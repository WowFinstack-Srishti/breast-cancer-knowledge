-> Breast Cancer Classification from Histopathology Images.
    -> Add Grad-CAM for explainability. (Later after base project)

â€¢	Data Availability: âœ… High (BreakHis or Camelyon16/17) 
    â€¢   BreakHis(https://www.kaggle.com/datasets/ambarish/breakhis)
    â€¢   Camelyon16/17(https://camelyon17.grand-challenge.org/)
â€¢	Skills Needed: Basic image classification using CNN
â€¢	Tools: Python, Keras/PyTorch, OpenCV
â€¢	SHAP adds statistical rigor; Grad-CAM adds visual appeal.

CNN:
-> basic image classification using Convolutional Neural Networks (CNNs) is a key skill in machine learning,
     especially for projects involving visual data like medical imaging, object recognition, etc.


ğŸ§  (1) What is Image Classification?

Image classification is a task where a model takes an input image and assigns it a label (category).
Examples:
	â€¢	Is this a cat or dog?
	â€¢	Is this X-ray normal or pneumonia?


ğŸ§± (2) Why CNN for Images?

CNNs (Convolutional Neural Networks) are designed to process pixel data. Unlike regular neural networks, CNNs use filters to detect features (edges, corners, patterns) in images.


âš™ï¸ (3) CNN Building Blocks

    ğŸ§© 1. Convolutional Layer
	    â€¢	Applies a filter (kernel) over the image to detect features.
	    â€¢	Think: edge detectors, texture detectors.

    ğŸ§© 2. Activation Function (ReLU)
	    â€¢	Introduces non-linearity to allow complex patterns.

    ğŸ§© 3. Pooling Layer (MaxPooling)
	    â€¢	Reduces the size (downsampling), retains key features.
	    â€¢	Helps reduce computation & overfitting.

    ğŸ§© 4. Flatten Layer
	    â€¢	Converts the 2D output into 1D vector to feed into dense layers.

    ğŸ§© 5. Fully Connected (Dense) Layer
	    â€¢	Final classification decision is made here.
	    â€¢	Often ends in a Softmax (for multiclass) or Sigmoid (for binary) layer.


ğŸ› ï¸ (4) Setup & Tools (for Mac M1)
    pip install tensorflow matplotlib numpy


ğŸ§ª (5) Simple CNN Example (MNIST Handwritten Digits)

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Preprocess
x_train = x_train.reshape(-1, 28, 28, 1).astype("float32") / 255
x_test = x_test.reshape(-1, 28, 28, 1).astype("float32") / 255

# Build CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train
history = model.fit(x_train, y_train, epochs=5, validation_split=0.1)

# Evaluate
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test accuracy:", test_acc)


ğŸ“Š 6. Evaluate the Model

Plot training history:

plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.legend()
plt.show()


ğŸ§¼ 7. Data Augmentation (Important for small datasets)

Helps prevent overfitting.

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    horizontal_flip=True
)

datagen.fit(x_train)

#Use model.fit(datagen.flow(x_train, y_train), ...) instead of regular training.

